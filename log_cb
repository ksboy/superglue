root@48153f4f99fc:/data/houwei/superglue# bash cb.sh
07/15/2019 11:11:02 - INFO - __main__ -   device: cuda n_gpu: 4, distributed training: False, 16-bits training: False
07/15/2019 11:11:02 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file ../bert-base-cased-finetuned-mrpc/vocab.txt
07/15/2019 11:11:02 - INFO - pytorch_pretrained_bert.modeling -   loading weights file ../bert-base-cased-finetuned-mrpc/pytorch_model.bin
07/15/2019 11:11:02 - INFO - pytorch_pretrained_bert.modeling -   loading configuration file ../bert-base-cased-finetuned-mrpc/config.json
07/15/2019 11:11:02 - INFO - pytorch_pretrained_bert.modeling -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 28996
}

07/15/2019 11:11:07 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
07/15/2019 11:11:09 - INFO - __main__ -   ***** Running training *****
07/15/2019 11:11:09 - INFO - __main__ -     Num examples = 250
07/15/2019 11:11:09 - INFO - __main__ -     Batch size = 32
07/15/2019 11:11:09 - INFO - __main__ -     Num steps = 24
Epoch: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:18<00:00,  7.44s/it]
  /15/2019 11:11:28 - INFO - pytorch_pretrained_bert.modeling -   loading weights file ./cb/pytorch_model.bin█████| 8/8 [00:03<00:00,  2.31it/s]
07/15/2019 11:11:28 - INFO - pytorch_pretrained_bert.modeling -   loading configuration file ./cb/config.json
07/15/2019 11:11:28 - INFO - pytorch_pretrained_bert.modeling -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 28996
}

07/15/2019 11:11:34 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file ./cb/vocab.txt
07/15/2019 11:11:34 - INFO - __main__ -   ***** Running evaluation *****
07/15/2019 11:11:34 - INFO - __main__ -     Num examples = 56
07/15/2019 11:11:34 - INFO - __main__ -     Batch size = 8
Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 21.91it/s]
07/15/2019 11:11:34 - INFO - __main__ -   ***** Eval results *****
07/15/2019 11:11:34 - INFO - __main__ -     acc = 0.7857142857142857
07/15/2019 11:11:34 - INFO - __main__ -     eval_loss = 0.6418886738164085
07/15/2019 11:11:34 - INFO - __main__ -     global_step = 24
07/15/2019 11:11:34 - INFO - __main__ -     loss = 0.138374046732982
