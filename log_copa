root@48153f4f99fc:/data/houwei/superglue# bash copa.sh
07/15/2019 12:39:34 - INFO - __main__ -   device: cuda n_gpu: 4, distributed training: False, 16-bits training: False
07/15/2019 12:39:34 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file ../bert-base-cased-finetuned-mrpc/vocab.txt
07/15/2019 12:39:35 - INFO - pytorch_pretrained_bert.modeling -   loading weights file ../bert-base-cased-finetuned-mrpc/pytorch_model.bin
07/15/2019 12:39:35 - INFO - pytorch_pretrained_bert.modeling -   loading configuration file ../bert-base-cased-finetuned-mrpc/config.json
07/15/2019 12:39:35 - INFO - pytorch_pretrained_bert.modeling -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 28996
}

07/15/2019 12:39:42 - INFO - __main__ -   ***** Running training *****
07/15/2019 12:39:42 - INFO - __main__ -     Num examples = 800
07/15/2019 12:39:42 - INFO - __main__ -     Batch size = 32
07/15/2019 12:39:42 - INFO - __main__ -     Num steps = 75
Epoch: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:41<00:00, 15.35s/it]
07/15/2019 12:40:25 - INFO - pytorch_pretrained_bert.modeling -   loading weights file ./copa/pytorch_model.bin██████████████████| 25/25 [00:10<00:00,  2.31it/s]
07/15/2019 12:40:25 - INFO - pytorch_pretrained_bert.modeling -   loading configuration file ./copa/config.json
07/15/2019 12:40:25 - INFO - pytorch_pretrained_bert.modeling -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 28996
}

07/15/2019 12:40:30 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file ./copa/vocab.txt
07/15/2019 12:40:30 - INFO - __main__ -   ***** Running evaluation *****
07/15/2019 12:40:30 - INFO - __main__ -     Num examples = 200
07/15/2019 12:40:30 - INFO - __main__ -     Batch size = 8
Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:01<00:00, 22.84it/s]
07/15/2019 12:40:32 - INFO - __main__ -   ***** Eval results *****
07/15/2019 12:40:32 - INFO - __main__ -     acc = 0.5
07/15/2019 12:40:32 - INFO - __main__ -     eval_loss = 0.69475270986557
07/15/2019 12:40:32 - INFO - __main__ -     global_step = 75
07/15/2019 12:40:32 - INFO - __main__ -     loss = 0.2323846928278605
